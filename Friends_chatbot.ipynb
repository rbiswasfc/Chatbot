{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Friends.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BLpMziLv3at7","colab_type":"text"},"source":["## Setup\n"]},{"cell_type":"code","metadata":{"id":"6BcO5y9T3Go1","colab_type":"code","outputId":"b5b16e49-d631-4a07-b796-0b6a2e434397","executionInfo":{"status":"ok","timestamp":1573705261992,"user_tz":-480,"elapsed":31359,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":161}},"source":["# connect to google drive\n","import os\n","# mount google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","#\n","# change root directory such that models are saved in google drive during training\n","root_dir = \"/content/gdrive/My Drive/Chatbot\"\n","os.chdir(root_dir)\n","# print the contents\n","!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","Chatbot        Chatbot.optim\t\t Friends.ipynb\tTutorial\n","Chatbot.ipynb  chatbot_refactored.ipynb  Friends.pkl\tvocab.json\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9SSFBpjK3nHS","colab_type":"text"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"KQ4Yu30mUNHj","colab_type":"code","colab":{}},"source":["# basic packages\n","import sys\n","import math\n","import time\n","import string\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TphoR_pd7iuU","colab_type":"code","colab":{}},"source":["pd.set_option('display.max_colwidth', 150)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l4owg0m03pdi","colab_type":"code","colab":{}},"source":["# export\n","# basic imports\n","import requests\n","from bs4 import BeautifulSoup\n","import seaborn as sns\n","from time import sleep\n","import re\n","import shutil\n","import json\n","from tqdm import tqdm_notebook\n","\n","import spacy\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pIOPjECJmmMy","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Diqy9ufz3px_","colab_type":"code","colab":{}},"source":["\n","from collections import Counter, namedtuple\n","from docopt import docopt\n","from itertools import chain\n","import json\n","from typing import List, Tuple, Dict, Set, Union\n","\n","#pytorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.utils\n","from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n","\n","\n","#others\n","from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n","from IPython.core.debugger import set_trace"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XbQORG43AS_v","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lzKc2jwpjFxx","colab_type":"text"},"source":["### load data"]},{"cell_type":"code","metadata":{"id":"lBwVpjF3EyT3","colab_type":"code","outputId":"2d0286ab-3647-43e1-af53-daa4ad86aff5","executionInfo":{"status":"ok","timestamp":1573705281684,"user_tz":-480,"elapsed":1486,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["df_friends = pd.read_pickle('Friends.pkl')\n","df_friends.head()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pre</th>\n","      <th>post</th>\n","      <th>is_valid</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>Monica: There's nothing to tell! He's just some guy\\nI work with!</td>\n","      <td>Joey: C'mon, you're going out with the guy! There's\\ngotta be something wrong with him!</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Joey: C'mon, you're going out with the guy! There's\\ngotta be something wrong with him!</td>\n","      <td>Chandler: All right Joey, be\\nnice.  So does he have a hump? A hump and a hairpiece?</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Chandler: All right Joey, be\\nnice.  So does he have a hump? A hump and a hairpiece?</td>\n","      <td>Phoebe: Wait, does he eat chalk?</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Phoebe: Just, 'cause, I don't want her to go through\\nwhat I went through with Carl- oh!</td>\n","      <td>Monica: Okay, everybody relax. This is not even a\\ndate. It's just two people going out to dinner and- not having sex.</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Monica: Okay, everybody relax. This is not even a\\ndate. It's just two people going out to dinner and- not having sex.</td>\n","      <td>Chandler: Sounds like a date to me.</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                                      pre  ... is_valid\n","2                                                       Monica: There's nothing to tell! He's just some guy\\nI work with!  ...     True\n","3                                 Joey: C'mon, you're going out with the guy! There's\\ngotta be something wrong with him!  ...     True\n","4                                    Chandler: All right Joey, be\\nnice.  So does he have a hump? A hump and a hairpiece?  ...     True\n","7                                Phoebe: Just, 'cause, I don't want her to go through\\nwhat I went through with Carl- oh!  ...     True\n","8  Monica: Okay, everybody relax. This is not even a\\ndate. It's just two people going out to dinner and- not having sex.  ...     True\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"GR2ephKUzsYj","colab_type":"code","colab":{}},"source":["def check_len(x):\n","  flag = True\n","  for sent in x:\n","    #set_trace()\n","    if len(sent) == 0:\n","      flag = False\n","  return flag\n","\n","df_friends['flag'] = df_friends[['pre', 'post']].apply(lambda x: check_len(x), axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wtl255231KRf","colab_type":"code","outputId":"4ac76061-0367-4d3c-b39e-bf7335fa0af1","executionInfo":{"status":"ok","timestamp":1573705301669,"user_tz":-480,"elapsed":717,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["df_friends = df_friends[df_friends['flag']].reset_index(drop = True)\n","len(df_friends)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["36735"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"tV_Iaskhsm3r","colab_type":"code","colab":{}},"source":["# handle contraction\n","contraction_mapping = {\"c'mon\": 'come on', \"there's\": 'there is', \"it's\": 'it is', \"you're\": 'you are',\n","                       \"he's\": 'he is', \"she's\": 'she is', \"i'm\": 'i am', \"don't\": 'do not', \"i've\": 'i have', \n","                       \"doesn't\": 'does not', \"didn't\": 'did not', \"you’re\": 'you are', \"i’m \": 'i am',\n","                       \"he’s\": 'he is', \"you've\": 'you have', \"let's\": 'let us'}\n","def correct_contraction(x, dic):\n","    for word in dic.keys():\n","        if word in x:\n","            x = x.replace(word, dic[word])\n","    return x\n","\n","# handle punctuation\n","all_punct = list(string.punctuation)\n","def spacing_punctuation(text):\n","    \"\"\"\n","    add space before and after punctuation and symbols\n","    \"\"\"\n","    for punc in all_punct:\n","        if punc in text:\n","            text = text.replace(punc, f' {punc} ')\n","    return text\n","\n","# put together\n","def clean_text(x):\n","  x = x.lower()\n","  x = re.sub(r\"\\n\", \" \", x) # remove \\n character\n","  x = correct_contraction(x, contraction_mapping)\n","  x = spacing_punctuation(x)\n","\n","  #words = x.split(\" \")\n","  #words = [word.strip() for word in words]\n","  return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZ8UZ_rjvKq_","colab_type":"code","colab":{}},"source":["df_friends['pre'] = df_friends['pre'].apply(lambda x: clean_text(x))\n","df_friends['post'] = df_friends['post'].apply(lambda x: clean_text(x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v016BddevSxt","colab_type":"code","outputId":"46f44032-d046-4110-aea0-4bf63e34b9ec","executionInfo":{"status":"ok","timestamp":1573705308188,"user_tz":-480,"elapsed":605,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["df_friends.sample(5)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pre</th>\n","      <th>post</th>\n","      <th>is_valid</th>\n","      <th>flag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>25423</th>\n","      <td>joey :  you know you’ve been spitting on me ?  !</td>\n","      <td>richard :  that’s what real actors do !  annunciation is the mark of a good actor !  and when you enunciate ,  you spit !   ( spits on the t )</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>18336</th>\n","      <td>monica :</td>\n","      <td>ross :</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>23750</th>\n","      <td>joey :  dude ,  you soooo need this car .</td>\n","      <td>phoebe :   ( running up )  okay .  okay ,  here’s what we’re gonna do .  okay ,  i amgonna break into this mini - van and put it in neutral .  you...</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>28262</th>\n","      <td>rachel :</td>\n","      <td>monica :</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>312</th>\n","      <td>monica :  what does she mean by  ' involved '  ?</td>\n","      <td>chandler :  i mean presumably ,  the biggest part of your job is done .</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                      pre  ...  flag\n","25423   joey :  you know you’ve been spitting on me ?  !   ...  True\n","18336                                           monica :   ...  True\n","23750          joey :  dude ,  you soooo need this car .   ...  True\n","28262                                          rachel :    ...  True\n","312    monica :  what does she mean by  ' involved '  ?    ...  True\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"U4RNCaJYMlSd","colab_type":"text"},"source":["### Data preparation"]},{"cell_type":"code","metadata":{"id":"BbWmGsJRMhgJ","colab_type":"code","colab":{}},"source":["# read from corpus: vocab building\n","def sent_tokenizer(sent, source):\n","\n","    words = sent.split(' ')\n","    words = [word.lower() for word in words]\n","    \n","    if source == 'post':\n","      words = ['<sos>'] + words + ['<eos>']\n","\n","    return words"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"woo0b-KEOnbm","colab_type":"code","colab":{}},"source":["# generate batches for taining\n","pre_sents, post_sents = [], []\n","\n","for i in range(len(df_friends)):\n","  pre_raw = df_friends.iloc[i].pre\n","  post_raw = df_friends.iloc[i].post\n","  \n","  pre_tokens =  sent_tokenizer(pre_raw, 'pre')\n","  post_tokens =  sent_tokenizer(post_raw, 'post')\n","  \n","  pre_sents.append(pre_tokens)\n","  post_sents.append(post_tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n4MiMvb9Mhjk","colab_type":"code","colab":{}},"source":["data = list(zip(pre_sents, post_sents))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDGo2bgdY47t","colab_type":"code","outputId":"7cd35aef-8b16-4dfb-9668-79ec480d61a9","executionInfo":{"status":"ok","timestamp":1573705341207,"user_tz":-480,"elapsed":8578,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":567}},"source":["data[110]"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['monica', ':', '', '', '-', 'leg', '?', ''],\n"," ['<sos>',\n","  'paul',\n","  ':',\n","  '',\n","  '',\n","  '(',\n","  'laughing',\n","  ')',\n","  '',\n","  'that',\n","  \"'\",\n","  's',\n","  'one',\n","  'way',\n","  '!',\n","  '',\n","  'me',\n","  ',',\n","  '',\n","  'i',\n","  '-',\n","  '',\n","  'i',\n","  'went',\n","  'for',\n","  'the',\n","  'watch',\n","  '.',\n","  '',\n","  '<eos>'])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"zpnpuDwY8oqj","colab_type":"text"},"source":["### padding"]},{"cell_type":"code","metadata":{"id":"a-EIYbAPQRlr","colab_type":"code","colab":{}},"source":["# post-padding for source/target sequences\n","def pad_sents(sents, pad_token):\n","    \n","    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n","    @param sents (list[list[str]]): list of sentences, where each sentence\n","                                    is represented as a list of words\n","    @param pad_token (str): padding token\n","    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n","        than the max length sentence are padded out with the pad_token, such that\n","        each sentences in the batch now has equal length.\n","    \"\"\"\n","    sents_padded = []\n","\n","    max_len = min(max([len(sent) for sent in sents]),100)\n","    for sent in sents:\n","        sent = sent[:max_len]\n","        sent_len = len(sent)\n","        sents_padded.append(sent + (max_len - sent_len) * [pad_token])\n","\n","    return sents_padded"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPWkzAktQR8L","colab_type":"text"},"source":["### Build vocab"]},{"cell_type":"code","metadata":{"id":"6NplKN5cMhmD","colab_type":"code","colab":{}},"source":["class Vocab(object):\n","    \"\"\" Construct vocabulary for chatbot.\n","    \"\"\"\n","    def __init__(self, word2id=None):\n","        \"\"\" Init Vocab Instance.\n","        @param word2id (dict): dictionary mapping words 2 indices\n","        \"\"\"\n","        if word2id:\n","            self.word2id = word2id\n","        else:\n","            self.word2id = dict()\n","            self.word2id['<pad>'] = 0   # Pad Token\n","            self.word2id['<sos>'] = 1 # Start Token\n","            self.word2id['<eos>'] = 2    # End Token\n","            self.word2id['<unk>'] = 3   # Unknown Token\n","        self.unk_id = self.word2id['<unk>']\n","        self.id2word = {v: k for k, v in self.word2id.items()}\n","\n","    def __getitem__(self, word):\n","        \"\"\" Retrieve word's index. Return the index for the unk\n","        token if the word is out of vocabulary.\n","        @param word (str): word to look up.\n","        @returns index (int): index of word \n","        \"\"\"\n","        return self.word2id.get(word, self.unk_id)\n","\n","    def __contains__(self, word):\n","        \"\"\" Check if word is captured by VocabEntry.\n","        @param word (str): word to look up\n","        @returns contains (bool): whether word is contained    \n","        \"\"\"\n","        return word in self.word2id\n","\n","    def __setitem__(self, key, value):\n","        \"\"\" Raise error, if one tries to edit the VocabEntry.\n","        \"\"\"\n","        raise ValueError('vocabulary is readonly')\n","\n","    def __len__(self):\n","        \"\"\" Compute number of words in VocabEntry.\n","        @returns len (int): number of words in VocabEntry\n","        \"\"\"\n","        return len(self.word2id)\n","\n","    def __repr__(self):\n","        \"\"\" Representation of VocabEntry to be used\n","        when printing the object.\n","        \"\"\"\n","        return 'Vocabulary[size=%d]' % len(self)\n","\n","    def id2word(self, wid):\n","        \"\"\" Return mapping of index to word.\n","        @param wid (int): word index\n","        @returns word (str): word corresponding to index\n","        \"\"\"\n","        return self.id2word[wid]\n","\n","    def add(self, word):\n","        \"\"\" Add word to VocabEntry, if it is previously unseen.\n","        @param word (str): word to add to VocabEntry\n","        @return index (int): index that the word has been assigned\n","        \"\"\"\n","        if word not in self:\n","            wid = self.word2id[word] = len(self)\n","            self.id2word[wid] = word\n","            return wid\n","        else:\n","            return self[word]\n","\n","    def words2indices(self, sents):\n","        \"\"\" Convert list of words or list of sentences of words\n","        into list or list of list of indices.\n","        @param sents (list[str] or list[list[str]]): sentence(s) in words\n","        @return word_ids (list[int] or list[list[int]]): sentence(s) in indices\n","        \"\"\"\n","        if type(sents[0]) == list:\n","            return [[self[w] for w in s] for s in sents]\n","        else:\n","            return [self[w] for w in sents]\n","\n","    def indices2words(self, word_ids):\n","        \"\"\" Convert list of indices into words.\n","        @param word_ids (list[int]): list of word ids\n","        @return sents (list[str]): list of words\n","        \"\"\"\n","        return [self.id2word[w_id] for w_id in word_ids]\n","\n","    def to_input_tensor(self, sents, device: torch.device):\n","        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n","        shorter sentences.\n","\n","        @param sents (List[List[str]]): list of sentences (words)\n","        @param device: device on which to load the tesnor, i.e. CPU or GPU\n","\n","        @returns sents_var: tensor of (max_sentence_length, batch_size)\n","        \"\"\"\n","        word_ids = self.words2indices(sents)\n","        sents_t = pad_sents(word_ids, self['<pad>'])\n","        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n","        return torch.t(sents_var)\n","\n","    def save(self, file_path):\n","        \"\"\" Save Vocab to file as JSON dump.\n","        @param file_path (str): file path to vocab file\n","        \"\"\"\n","        json.dump(dict(vocab_word2id=self.word2id), open(file_path, 'w'), indent=2)\n","\n","    @staticmethod\n","    def load(file_path):\n","        \"\"\" Load vocabulary from JSON dump.\n","        @param file_path (str): file path to vocab file\n","        @returns Vocab object loaded from JSON dump\n","        \"\"\"\n","        entry = json.load(open(file_path, 'r'))\n","        word2id = entry['vocab_word2id']\n","        vocab = Vocab(word2id)\n","        print('{} loaded!'.format(vocab))\n","        return vocab\n","\n","    @staticmethod\n","    def from_corpus(corpus, size, freq_cutoff=2):\n","        \"\"\" Given a corpus construct a Vocab Entry.\n","        @param corpus (list[str]): corpus of text produced by read_corpus function\n","        @param size (int): # of words in vocabulary\n","        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n","        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n","        \"\"\"\n","        vocab_entry = Vocab()\n","        word_freq = Counter(chain(*corpus))\n","        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n","        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n","              .format(len(word_freq), freq_cutoff, len(valid_words)))\n","        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n","        for word in top_k_words:\n","            vocab_entry.add(word)\n","        return vocab_entry"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_3p1Flnx-p9K","colab_type":"text"},"source":["#### unit test"]},{"cell_type":"code","metadata":{"id":"Y3ZhmNBPRQsG","colab_type":"code","outputId":"8783141a-d5bb-465a-c3d8-5c6565f87922","executionInfo":{"status":"ok","timestamp":1573706057425,"user_tz":-480,"elapsed":1200,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["# Check Vocab Object\n","lang = Vocab()\n","print('Check if <eos> token is inside vocab:')\n","print('<eos>' in lang) #__contains__#\n","print('Length of vocab:{}'.format(len(lang))) # __len__\n","print('Adding a new word')\n","lang.add('new') # add new entry 'add' method\n","print(lang) #__repr__\n","print('The index of \"new\" is:{}'.format(lang.word2id['new']))\n","## \n","print('Generate a vocab with the from_corpus static method:')\n","en_vocab = Vocab.from_corpus(post_sents, 100)\n","print('The token for the word \"the\" is:{}'.format(en_vocab.word2id['the']))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Check if <eos> token is inside vocab:\n","True\n","Length of vocab:4\n","Adding a new word\n","Vocabulary[size=5]\n","The index of \"new\" is:4\n","Generate a vocab with the from_corpus static method:\n","number of word types: 14179, number of word types w/ frequency >= 2: 7568\n","The token for the word \"the\" is:12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qKnztD0SSiEA","colab_type":"code","outputId":"4f152e36-4134-4116-86c7-25f970b222ca","executionInfo":{"status":"ok","timestamp":1573706059988,"user_tz":-480,"elapsed":1624,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["# Save and load vocab\n","vocab_file = 'vocab.json'\n","size = 7500 # number of word tokens in vocab\n","freq_cutoff= 2\n","vocab = Vocab.from_corpus(pre_sents, size, freq_cutoff)\n","print('generated vocabulary, %d words' % (len(vocab)))\n","\n","vocab.save(vocab_file)\n","print('vocabulary saved to %s' % vocab_file)\n","en_vocab = Vocab.load(vocab_file)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["number of word types: 14125, number of word types w/ frequency >= 2: 7620\n","generated vocabulary, 7504 words\n","vocabulary saved to vocab.json\n","Vocabulary[size=7504] loaded!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9p7qoTYDq0v2","colab_type":"code","outputId":"ab9212dd-ae16-4971-bfb3-52217e3679da","executionInfo":{"status":"ok","timestamp":1573706061511,"user_tz":-480,"elapsed":618,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(en_vocab)"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7504"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"xQzQD7PkAF3m","colab_type":"text"},"source":["### train-test split"]},{"cell_type":"code","metadata":{"id":"k0TWkZ7rTFGq","colab_type":"code","outputId":"7c9a3124-92a0-4541-d202-72a0dacd888f","executionInfo":{"status":"ok","timestamp":1573706065964,"user_tz":-480,"elapsed":1192,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["# load data\n","train_data, valid_data = train_test_split(data, test_size = 0.15, random_state = 42)\n","#\n","print(\"==\"*40)\n","print(\"Number of examples in train: {}\".format(len(train_data)))\n","print(\"Number of examples in valid: {}\".format(len(valid_data)))\n","#\n","print(\"==\"*40)\n","print(\"Chatbot\")\n","query, answer = next(iter(train_data))\n","print(\"Q: {}\".format(' '.join(query)))\n","print(\"A: {}\".format(' '.join(answer)))\n","print(\"==\"*40)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["================================================================================\n","Number of examples in train: 31224\n","Number of examples in valid: 5511\n","================================================================================\n","Chatbot\n","Q: rachel :  uh - huh . \n","A: <sos> ross :  we live together .  you are having our baby .  i amnot gonna see anybody else .  are you - are you sure you don’t want something more ?  <eos>\n","================================================================================\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BDi_aqrfj9bY","colab_type":"text"},"source":["### Embeddings"]},{"cell_type":"code","metadata":{"id":"MJJEgiUyTFMZ","colab_type":"code","colab":{}},"source":["class ModelEmbeddings(nn.Module): \n","    \"\"\"\n","    Class that converts input words to their embeddings.\n","    \"\"\"\n","    def __init__(self, vocab, embed_size=256):\n","        \"\"\"\n","        Init the Embedding layers.\n","\n","        @param embed_size (int): Embedding size (dimensionality)\n","        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n","                              See vocab.py for documentation.\n","        \"\"\"\n","        super(ModelEmbeddings, self).__init__()\n","        self.embed_size = embed_size\n","\n","        pad_token_idx = vocab['<pad>']\n","        \n","        self.LUT = nn.Embedding(len(vocab), embed_size, padding_idx = pad_token_idx)\n","\n","    def forward(self, x):\n","      \"\"\" Get the embedding for x\n","      @ param x: tokes (batch, seq_len)\n","      @ return x_embed (batch, seq_len, x_embed)\n","      \"\"\"\n","      x_embed = self.LUT(x)*math.sqrt(self.embed_size)\n","      return x_embed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zf1UxBY_FCV8","colab_type":"code","colab":{}},"source":["embed_size = 256\n","vocab = en_vocab\n","en_embeddings = ModelEmbeddings(vocab, embed_size)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r4ifFTqsGQnY","colab_type":"text"},"source":["### Encoder"]},{"cell_type":"code","metadata":{"id":"F_ukJaj0BHv9","colab_type":"code","colab":{}},"source":["class ChatbotEncoder(nn.Module):\n","  \"\"\" encodes query sentence \"\"\"\n","  def __init__(self, en_embeddings, hidden_size, dropout_rate):\n","    super(ChatbotEncoder, self).__init__()\n","\n","    # attributes\n","    self.hidden_size = hidden_size\n","    embed_size = en_embeddings.embed_size\n","\n","    # layers\n","    self.embed = en_embeddings\n","    self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n","    self.dropout = nn.Dropout(dropout_rate)\n","    self.h_projection= nn.Linear(2*hidden_size, hidden_size, bias=False)\n","    self.c_projection = nn.Linear(2*hidden_size, hidden_size, bias=False)\n","\n","  def generate_mask(self, source_padded, source_lengths):\n","    \"\"\" \n","    Generate sentence masks for encoder hidden states. Applies mask for padded inputs.\n","    param:\n","    source_padded: tensor (max_len, batch)\n","    source_lengths: List containing lengths of input sentences\n","\n","    returns:\n","    enc_masks: Tensor (batch, src_len)\n","    \"\"\"\n","    \n","\n","    max_len, batch = source_padded.shape[0], source_padded.shape[1]\n","    enc_masks = torch.zeros(batch, max_len, dtype = torch.float)\n","    \n","    for e_id, length in enumerate(source_lengths):\n","      enc_masks[e_id, length: ] = 1\n","\n","    return enc_masks.bool().to(self.device)\n","\n","    \n","  def forward(self, source_padded, source_lengths):\n","    \"\"\" \n","    Apply the encoder to source sentences to obtain encoder hidden states.\n","    Take the final states of the encoder and project them to obtain initial states for decoder.\n","    params:\n","    source_padded: Tensor (src_len, b)\n","    source_lengths: List containing original sentence lengths \n","    \"\"\"\n","    \n","    enc_masks = self.generate_mask(source_padded, source_lengths) # (batch, src_len) # to be used in decoder\n","    X = self.embed(source_padded) #(src_len, b, embed_size)\n","    X = pack_padded_sequence(X, source_lengths)\n","    enc_hiddens, (last_hidden,last_cell) = self.lstm(X) #(h0,c0) defaults to zero\n","    enc_hiddens, _ = pad_packed_sequence(enc_hiddens, batch_first=True)\n","    # enc_hiddens: (batch, src_len, num_directions*hidden_size)\n","    # last_hidden: (num_dim*n_layes, batch, hidden_size)\n","    last_hidden = torch.cat((last_hidden[0,:],last_hidden[1,:]),1) #(batch, 2*hidden_size)\n","    last_cell = torch.cat((last_cell[0,:],last_cell[1,:]),1) #(batch, 2*hidden_size)\n","    #set_trace()\n","    init_decoder_hidden = self.h_projection(last_hidden) #(batch, hidden_size)\n","    init_decoder_cell = self.c_projection(last_cell) #(batch, hidden_size)\n","    dec_init_state = (init_decoder_hidden, init_decoder_cell)\n","\n","    return enc_hiddens, enc_masks, dec_init_state\n","\n","  @property\n","  def device(self) -> torch.device:\n","      \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n","      \"\"\"\n","      return self.embed.LUT.weight.device"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6XjCekQOn0mp","colab_type":"code","colab":{}},"source":["encoder = ChatbotEncoder(en_embeddings, hidden_size = 128, dropout_rate = 0.3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"426ocybejf-d","colab_type":"code","colab":{}},"source":["query = [['i', 'am', 'going', 'there', 'today'], ['it', 'is', 'not', 'possible', 'to', 'do', 'this', 'in', 'such', 'short', 'time'], ['hey', 'amigo']]\n","query_sorted = sorted(query, key= lambda e: len(e), reverse=True)\n","query_lengths = [len(sent) for sent in query_sorted]\n","query_padded = en_vocab.to_input_tensor(query_sorted, device = 'cpu') # (src_len, batch)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hfGlf0mts8BR","colab_type":"code","outputId":"68b6bc63-9ed0-4b48-a049-1b5a2cf01c46","executionInfo":{"status":"ok","timestamp":1573706083244,"user_tz":-480,"elapsed":1168,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":212}},"source":["query_padded"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[  23,    9,   50],\n","        [  24,   59,    3],\n","        [  31,  100,    0],\n","        [1433,   82,    0],\n","        [  13,  315,    0],\n","        [  33,    0,    0],\n","        [  35,    0,    0],\n","        [  38,    0,    0],\n","        [ 401,    0,    0],\n","        [1315,    0,    0],\n","        [ 115,    0,    0]])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"0TVRvIHEqEVE","colab_type":"code","colab":{}},"source":["enc_hiddens, enc_masks, dec_init_state = encoder(query_padded, query_lengths)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kRmUXHmF2xtT","colab_type":"code","outputId":"1d6108d1-1c53-45c2-f0c5-b6e867f74dff","executionInfo":{"status":"ok","timestamp":1573706086626,"user_tz":-480,"elapsed":697,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["print(enc_masks)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["tensor([[False, False, False, False, False, False, False, False, False, False,\n","         False],\n","        [False, False, False, False, False,  True,  True,  True,  True,  True,\n","          True],\n","        [False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n","          True]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"06gZy8uUqDFC","colab_type":"code","outputId":"d85f37e3-bea4-4b9a-8967-94da9d1527bb","executionInfo":{"status":"ok","timestamp":1573706089418,"user_tz":-480,"elapsed":1176,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["enc_hiddens.shape"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 11, 256])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"Gd2T-W_8sSiX","colab_type":"code","outputId":"2e520a53-4ada-47c3-ee53-98d10c312a47","executionInfo":{"status":"ok","timestamp":1573706090585,"user_tz":-480,"elapsed":656,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["dec_init_state[0].shape"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 128])"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"Kme_4D4p3SXR","colab_type":"code","outputId":"f14ae802-4922-4e24-9c0e-de7d83e8896c","executionInfo":{"status":"ok","timestamp":1573706092163,"user_tz":-480,"elapsed":706,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["encoder.device"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"lCRYssiIC0SW","colab_type":"text"},"source":["### Attention Mechanism"]},{"cell_type":"code","metadata":{"id":"HEdnhjfmC3nL","colab_type":"code","colab":{}},"source":["class GlobalAttention(nn.Module):\n","  \"\"\"\n","  Performs global attention mechanism\n","  \"\"\"\n","  def __init__(self, hidden_size):\n","    super(GlobalAttention, self).__init__()\n","\n","    self.hidden_size = hidden_size\n","    \n","    # multipolicative attention\n","    self.mult_atten = nn.Linear(hidden_size, hidden_size)\n","\n","  \n","  def forward(self, enc_hiddens_proj, dec_hidden, enc_masks =None):\n","    \"\"\"\n","    Return the softmax normalized probability scores\n","    \n","    params:\n","    enc_hiddens_proj: Tensor (batch, src_len, hidden_size)\n","    dec_hiddens: Tensor (batch, hidden_size)\n","    enc_masks: Bool Tensor (batch, src_len)\n","\n","    Return:\n","    alpha_t: Tensor (batch, src_len)\n","    \"\"\"\n","    assert self.hidden_size == dec_hidden.shape[1], \\\n","    \"Decoder LSTM hidden size and Linear layer output of mult_atten size must match\"\n","\n","    assert self.hidden_size == enc_hiddens_proj.shape[2], \\\n","    \"Encoder output projection size and Linear layer output of mult_atten size must match\"\n","    aug_dec_hidden = torch.unsqueeze(dec_hidden, dim=2) #(batch, hidden_size, 1)\n","    mul_enc_proj = self.mult_atten(enc_hiddens_proj) # (batch, src_len, hidden_size)\n","    e_t = torch.bmm(mul_enc_proj, aug_dec_hidden) \n","    # (batch, src_len, hidden_size) * (batch,hidden_size, 1) --> (batch, src_len, 1)\n","    e_t = torch.squeeze(e_t, dim=2) #(batch, src_len)\n","\n","    # masked attention\n","    if enc_masks is not None:\n","      e_t.data.masked_fill_(enc_masks.byte(), -float('inf'))\n","    # compute attention scores\n","    alpha_t = F.softmax(e_t, dim=1) #(batch, src_len)\n","    \n","    return alpha_t\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pF2evaw9C3k8","colab_type":"code","colab":{}},"source":["attn_mech = GlobalAttention(hidden_size= 5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mAv26H41C3iN","colab_type":"code","colab":{}},"source":["enc_hiddens_proj = torch.randn((3, 2, 5))\n","dec_hidden = torch.randn((3, 5))\n","alpha = attn_mech(enc_hiddens_proj, dec_hidden)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sSKdYEFmHlRn","colab_type":"code","outputId":"b2d1843b-6c3f-47f6-c795-5c63bbdd0753","executionInfo":{"status":"ok","timestamp":1573706101595,"user_tz":-480,"elapsed":851,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["alpha"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.1764, 0.8236],\n","        [0.4260, 0.5740],\n","        [0.2992, 0.7008]], grad_fn=<SoftmaxBackward>)"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"jlePpQEWF-Y-","colab_type":"text"},"source":["### Decoder"]},{"cell_type":"code","metadata":{"id":"7EYcfFAMGA6w","colab_type":"code","colab":{}},"source":["class ChatbotDecoder(nn.Module):\n","  def __init__(self, en_embeddings, hidden_size, attn_mech, dropout_rate):\n","    super(ChatbotDecoder, self).__init__()\n","    \n","    # attributes\n","    self.hidden_size = hidden_size\n","    embed_size = en_embeddings.embed_size\n","    self.attention = attn_mech\n","    \n","    # layers\n","    self.embed = en_embeddings\n","    self.lstm_decode = nn.LSTMCell(embed_size + hidden_size, hidden_size)\n","    self.att_projection = nn.Linear(2*hidden_size, hidden_size, bias=False)\n","    self.combined_output_projection = nn.Linear(3*hidden_size, hidden_size, bias=False)\n","    self.dropout = nn.Dropout(dropout_rate)\n","    \n","\n","  def forward(self, enc_hiddens, enc_masks, dec_init_state, target_padded):\n","    \"\"\"\n","    params:\n","    target_padded: Tensor (tgt_len, batch)\n","    \"\"\"\n","    # Chop of the <eos> token for max length sentences\n","    target_padded = target_padded[:-1] # (tgt_len-1, batch)\n","    # Initialize the decoder state (hidden and cell)\n","    dec_state = dec_init_state\n","    # Initialize previous combined output vector o_{t-1} as zero\n","    batch_size = enc_hiddens.shape[0]\n","    o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n","    # Initialize a list we will use to collect the combined output o_t on each step\n","    combined_outputs = []\n","\n","    # compute enc_hiddens projection\n","    enc_hiddens_proj = self.att_projection(enc_hiddens) # (batch, src_len, hidden_size)\n","\n","    # teacher-forcing\n","    Y = self.embed(target_padded) # (tgt_len, batch, embed_size)\n","    Y_splited = torch.split(Y, 1, dim=0) # tuple of size tgt_len containing Tensors of shape (1, batch, embed_size)\n","    tgt_len = target_padded.shape[0]\n","    #\n","    for i in range(tgt_len):\n","      Y_t = Y_splited[i] # Tensor (1, batch, embed_size)\n","      Y_t = torch.squeeze(Y_t, dim = 0) # Tensor (batch, embed_size)\n","      Ybar_t = torch.cat((Y_t, o_prev), dim = 1) # Tensor (batch, embed_size + hidden_size)\n","      dec_state = self.lstm_decode(Ybar_t, dec_state) # tuple of (dec_hidden, dec_cell) Tensors of shape (batch, hidden_size)\n","      dec_hidden, dec_cell = dec_state # (batch, hidden_size)\n","\n","      # Global Attention Mechanisms\n","      alpha_t = self.attention(enc_hiddens_proj, dec_hidden, enc_masks) \n","      \n","      # Compute attention vector\n","      aug_att = torch.unsqueeze(alpha_t,2) #(batch, src_len, 1)\n","      tr_hiddens = enc_hiddens.transpose(1,2) #(batch, hidden_size*2, src_len)\n","      a_t = torch.bmm(tr_hiddens, aug_att) #(batch, 2*hidden_size, 1)\n","      a_t = torch.squeeze(a_t, dim=2) #(batch, 2*hidden_size) # Attention vector\n","      \n","      # Compute combined output\n","      U_t = torch.cat((a_t, dec_hidden), dim=1) #(batch, 3*hidden_size)\n","      V_t = self.combined_output_projection(U_t) #(batch, hidden_size)\n","      O_t = self.dropout(torch.tanh(V_t)) # (batch, hidden_size)\n","      combined_outputs.append(O_t)\n","\n","    # compute overall combined outputs\n","    combined_outputs = torch.stack(combined_outputs)  # (tgt_len, batch, hidden_size)\n","    return combined_outputs\n","\n","  @property\n","  def device(self) -> torch.device:\n","      \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n","      \"\"\"\n","      return self.embed.LUT.weight.device\n","   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gQpM0CyGBC1","colab_type":"code","colab":{}},"source":["attn_mech = GlobalAttention(128)\n","decoder = ChatbotDecoder(en_embeddings, 128, attn_mech, 0.9)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xvsQOWd2LXib","colab_type":"code","colab":{}},"source":["target = [['<sos>', 'i', 'came', 'from', 'there', '<eos>'], ['<sos>', 'it', 'is', 'not', 'possible', '<eos>'], ['<sos>', '<eos>']]\n","target_padded = en_vocab.to_input_tensor(target, device = 'cpu') # (src_len, batch)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zuh5MVMN-sqM","colab_type":"code","colab":{}},"source":["combined_outputs = decoder(enc_hiddens, enc_masks, dec_init_state, target_padded)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PfS0dXrlL74K","colab_type":"code","outputId":"5e578380-4c1a-4d8d-c99e-ad2c9f21188b","executionInfo":{"status":"ok","timestamp":1573706113092,"user_tz":-480,"elapsed":1053,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["combined_outputs.shape"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 3, 128])"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"dzpCJgxXDsl9","colab_type":"text"},"source":["### Chatbot"]},{"cell_type":"code","metadata":{"id":"Kis7eJozDvHD","colab_type":"code","colab":{}},"source":["class Chatbot(nn.Module):\n","  \"\"\"build chatbot with encode and decoder \"\"\"\n","  def __init__(self, vocab, embed_size = 256, hidden_size = 256, dropout_rate = 0.3):\n","    super(Chatbot, self).__init__()\n","\n","    # attributes\n","    self.embed_size = embed_size\n","    self.hidden_size = hidden_size\n","    self.dropout_rate = dropout_rate\n","    self.vocab = vocab\n","\n","    # layers\n","    self.master_embeddings = ModelEmbeddings(vocab, embed_size)\n","    self.encoder = ChatbotEncoder(self.master_embeddings, hidden_size, dropout_rate)\n","    self.attn_mech = GlobalAttention(hidden_size)\n","    self.decoder = ChatbotDecoder(self.master_embeddings, hidden_size, self.attn_mech, dropout_rate)\n","    self.vocab_projection = nn.Linear(hidden_size, len(vocab), bias=False)\n","    self.dropout = nn.Dropout(dropout_rate)\n","\n","\n","  def forward(self, query, target):\n","    \"\"\"\n","    computes loss during training\n","    \"\"\"\n","    query_lengths = [len(s) for s in query]\n","    query_lengths = [min(x,100) for x in query_lengths]\n","\n","    # Convert list of lists into tensors\n","    query_padded = self.vocab.to_input_tensor(query, device=self.device)   # Tensor: (src_len, batch)\n","    target_padded = self.vocab.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, batch)\n","\n","    # encoder\n","    enc_hiddens, enc_masks, dec_init_state = self.encoder(query_padded, query_lengths)\n","    # decoder\n","    combined_outputs = self.decoder(enc_hiddens, enc_masks, dec_init_state, target_padded)\n","    # predicted target words\n","    pred_scores_raw = self.vocab_projection(combined_outputs)\n","    # softmax probabilities\n","    P = F.log_softmax(pred_scores_raw, dim=-1)\n","    \n","    # Zero out, probabilities for which we have nothing in the target text\n","    target_masks = (target_padded != self.vocab['<pad>']).float()\n","\n","    # Compute log probability of generating true target words\n","    target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]\n","    scores = target_gold_words_log_prob.sum(dim=0)\n","    return scores\n","  \n","  @property\n","  def device(self) -> torch.device:\n","      \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n","      \"\"\"\n","      return self.master_embeddings.LUT.weight.device\n","\n","  @staticmethod\n","  def load(model_path: str):\n","      \"\"\" Load the model from a file.\n","      @param model_path (str): path to model\n","      \"\"\"\n","      params = torch.load(model_path, map_location=lambda storage, loc: storage)\n","      args = params['args']\n","      model = Chatbot(vocab=params['vocab'], **args)\n","      model.load_state_dict(params['state_dict'])\n","\n","      return model\n","\n","  def save(self, path: str):\n","      \"\"\" Save the odel to a file.\n","      @param path (str): path to the model\n","      \"\"\"\n","      print('save model parameters to [%s]' % path, file=sys.stderr)\n","\n","      params = {\n","          'args': dict(embed_size=self.embed_size, hidden_size=self.hidden_size, dropout_rate=self.dropout_rate),\n","          'vocab': self.vocab,\n","          'state_dict': self.state_dict()\n","      }\n","\n","      torch.save(params, path)\n","\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rZO4NhyNDvKY","colab_type":"code","colab":{}},"source":["chatbot = Chatbot(en_vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SVRqNCUaDvSi","colab_type":"code","colab":{}},"source":["scores = chatbot(query_sorted, target)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Om56008iDvPq","colab_type":"code","outputId":"30dccba1-1b9e-4c76-8f07-90189a64a7e2","executionInfo":{"status":"ok","timestamp":1573706124452,"user_tz":-480,"elapsed":1742,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["scores"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-44.5802, -44.9359,  -9.0270], grad_fn=<SumBackward1>)"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"joTwzCVuRIQi","colab_type":"text"},"source":["### Data Loader\n"]},{"cell_type":"code","metadata":{"id":"Nxh83g8QDvNv","colab_type":"code","colab":{}},"source":["# generate batches for taining\n","\n","def batch_iter(data, batch_size, shuffle=False):\n","    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n","    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n","    @param batch_size (int): batch size\n","    @param shuffle (boolean): whether to randomly shuffle the dataset\n","    \"\"\"\n","    batch_num = math.ceil(len(data) / batch_size)\n","    index_array = list(range(len(data)))\n","\n","    if shuffle:\n","        np.random.shuffle(index_array)\n","\n","    for i in range(batch_num):\n","        indices = index_array[i * batch_size: (i + 1) * batch_size]\n","        examples = [data[idx] for idx in indices]\n","\n","        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n","        src_sents = [e[0] for e in examples]\n","        tgt_sents = [e[1] for e in examples]\n","\n","        yield src_sents, tgt_sents"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-BokBptvRQNR","colab_type":"text"},"source":["### Evaluation metric"]},{"cell_type":"code","metadata":{"id":"LB-HpRKDCzAB","colab_type":"code","colab":{}},"source":["## Compute Perplexity to keep track of training\n","def evaluate_ppl(model, dev_data, batch_size=16):\n","    \"\"\" Evaluate perplexity on dev sentences\n","    @param model (Chatbot): Chatbot Model\n","    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n","    @param batch_size (batch size)\n","    @returns ppl (perplixty on dev sentences)\n","    \"\"\"\n","    was_training = model.training\n","    model.eval()\n","\n","    cum_loss = 0.\n","    cum_tgt_words = 0.\n","\n","    # no_grad() signals backend to throw away all gradients\n","    with torch.no_grad():\n","        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n","            loss = -model(src_sents, tgt_sents).sum()\n","\n","            cum_loss += loss.item()\n","            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n","            cum_tgt_words += tgt_word_num_to_predict\n","\n","        ppl = np.exp(cum_loss / cum_tgt_words)\n","\n","    if was_training:\n","        model.train()\n","\n","    return ppl"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E_KIucYtRZd5","colab_type":"text"},"source":["### Training Loop"]},{"cell_type":"code","metadata":{"id":"69VI-uZ5CzDn","colab_type":"code","colab":{}},"source":["######## Train Model ########\n","\n","model_save_path = 'Chatbot'\n","\n","##\n","def train_model(model, optimizer, clip_grad =5.0, max_epoch =30, max_patience = 3, \n","                max_trial = 3, lr_decay = 0.5, train_batch_size = 128, log_every = 50, valid_niter = 200):\n","  \n","  \n","  print('Training begins...')\n","  ## Temp variables\n","  num_trial = 0\n","  train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n","  cum_examples = report_examples  = valid_num = 0\n","  hist_valid_scores = []\n","  train_time = begin_time = time.time()\n","  \n","  # put the model in training mode\n","  model.train()\n","  \n","  \n","  # iterate over the epochs\n","  for epoch in range(max_epoch):\n","    for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n","        \n","        train_iter += 1\n","        optimizer.zero_grad()\n","        batch_size = len(src_sents)\n","        \n","        example_losses = -model(src_sents, tgt_sents)\n","        batch_loss = example_losses.sum()\n","        loss = batch_loss/batch_size\n","        loss.backward() # autograd\n","        \n","        # Clip gradient\n","        grad_norn = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n","        optimizer.step() # update parameters\n","        \n","        batch_losses_val = batch_loss.item()\n","        report_loss += batch_losses_val\n","        cum_loss += batch_losses_val\n","        \n","        tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n","        report_tgt_words += tgt_words_num_to_predict\n","        cum_tgt_words += tgt_words_num_to_predict\n","        report_examples += batch_size\n","        cum_examples += batch_size\n","        \n","        # print interim report about training\n","        \n","        if train_iter % log_every == 0:\n","            #set_trace()\n","            print('| Epoch %d, Iter %d| Avg Loss = %.2f| Avg. ppl = %.2f| Speed %.2f words/sec| Time %.2f min|' \n","                  % (epoch+1, train_iter, report_loss / report_examples, math.exp(report_loss / report_tgt_words),\n","                          report_tgt_words / (time.time() - train_time), (time.time() - begin_time)/60.0))\n","\n","            train_time = time.time()\n","            report_loss = report_tgt_words = report_examples = 0.\n","        \n","        # validation\n","        if train_iter % valid_niter == 0:\n","            \n","            print('| <Train Summary> | Epoch %d, Iter %d| Cum. loss = %.2f| Cum. ppl = %.2f|' \n","                  % (epoch+1, train_iter, cum_loss / cum_examples, np.exp(cum_loss / cum_tgt_words)))\n","\n","            cum_loss = cum_examples = cum_tgt_words = 0.\n","            valid_num += 1\n","\n","            print('Report on validation set:', file=sys.stderr)\n","\n","            # compute dev. ppl and bleu\n","            dev_ppl = evaluate_ppl(model, valid_data, batch_size=128)   # dev batch size can be a bit larger\n","            valid_metric = -dev_ppl\n","\n","            print('Validation:  Dev. ppl = %f' % (dev_ppl), file=sys.stderr)\n","\n","            \n","            # learning rate scheduling\n","            \n","            is_better = (len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores))\n","            hist_valid_scores.append(valid_metric)\n","\n","            if is_better:\n","                patience = 0\n","                print('Save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n","                model.save(model_save_path)\n","\n","                # also save the optimizers' state\n","                torch.save(optimizer.state_dict(), model_save_path + '.optim')\n","                \n","            elif patience < int(max_patience):\n","                patience += 1\n","                print('Hit patience %d' % patience, file=sys.stderr)\n","\n","                if patience == int(max_patience):\n","                    num_trial += 1\n","                    print('Hit #%d trial' % num_trial, file=sys.stderr)\n","                    \n","                    if num_trial == int(max_trial):\n","                        print('early stop!', file=sys.stderr)\n","                        return\n","\n","                    # decay lr, and restore from previously best checkpoint\n","                    lr = optimizer.param_groups[0]['lr'] * float(lr_decay)\n","                    print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n","\n","                    # load model\n","                    params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n","                    model.load_state_dict(params['state_dict'])\n","                    model = model.to(device)\n","\n","                    print('restore parameters of the optimizers', file=sys.stderr)\n","                    optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n","\n","                    # set new lr\n","                    for param_group in optimizer.param_groups:\n","                        param_group['lr'] = lr\n","\n","                    # reset patience\n","                    patience = 0\n","\n","            if epoch +1 == int(max_epoch):\n","                print('Training stopped <-> Reached maximum number of epochs!', file=sys.stderr)\n","                return"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QotFKjPuRoBk","colab_type":"text"},"source":["### Model initialization"]},{"cell_type":"code","metadata":{"id":"wwvM2AZ1CzHb","colab_type":"code","colab":{}},"source":["model = Chatbot(en_vocab)\n","## Model in training mode\n","model.train();"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4vAe_fQoCzMx","colab_type":"code","outputId":"54d938c9-3e01-46e1-bbb5-2d4615056cbf","executionInfo":{"status":"ok","timestamp":1573706148536,"user_tz":-480,"elapsed":8701,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["## Parameter Initialization\n","uniform_init = 0.1\n","\n","def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -uniform_init, uniform_init)\n","        \n","model.apply(init_weights);\n","# Count total parameters\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')\n","\n","# Use Adam Optimizaer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# transfer model to cuda if available\n","device = torch.device(\"cuda:0\" if torch.cuda.device_count()>0 else \"cpu\")\n","print('use device: %s' % device)\n","model = model.to(device)"],"execution_count":53,"outputs":[{"output_type":"stream","text":["The model has 6,338,816 trainable parameters\n","use device: cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hYEfBtd1S9G2","colab_type":"text"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"3pKNt_HhBH50","colab_type":"code","outputId":"9cd3e78e-f37c-4980-f192-f198618e0ea6","executionInfo":{"status":"ok","timestamp":1573674157900,"user_tz":-480,"elapsed":2019641,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# train parameters\n","max_epoch = 10\n","train_batch_size = 64\n","\n","# train the model\n","train_model(model, optimizer, max_epoch = max_epoch, train_batch_size = train_batch_size)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training begins...\n","| Epoch 1, Iter 50| Avg Loss = 107.15| Avg. ppl = 118.59| Speed 3543.51 words/sec| Time 0.34 min|\n","| Epoch 1, Iter 100| Avg Loss = 83.25| Avg. ppl = 36.99| Speed 3635.75 words/sec| Time 0.68 min|\n","| Epoch 1, Iter 150| Avg Loss = 75.90| Avg. ppl = 27.48| Speed 3589.32 words/sec| Time 1.02 min|\n","| Epoch 1, Iter 200| Avg Loss = 74.39| Avg. ppl = 21.18| Speed 3713.16 words/sec| Time 1.37 min|\n","| <Train Summary> | Epoch 1, Iter 200| Cum. loss = 85.17| Cum. ppl = 39.35|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 21.943671\n","Save currently the best model to [Chatbot]\n","save model parameters to [Chatbot]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 1, Iter 250| Avg Loss = 71.73| Avg. ppl = 20.54| Speed 2533.10 words/sec| Time 1.87 min|\n","| Epoch 1, Iter 300| Avg Loss = 70.35| Avg. ppl = 22.44| Speed 3575.69 words/sec| Time 2.20 min|\n","| Epoch 1, Iter 350| Avg Loss = 69.66| Avg. ppl = 20.52| Speed 3601.92 words/sec| Time 2.54 min|\n","| Epoch 1, Iter 400| Avg Loss = 70.13| Avg. ppl = 16.61| Speed 3835.78 words/sec| Time 2.89 min|\n","| <Train Summary> | Epoch 1, Iter 400| Cum. loss = 70.47| Cum. ppl = 19.83|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 18.744193\n","Save currently the best model to [Chatbot]\n","save model parameters to [Chatbot]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 1, Iter 450| Avg Loss = 67.67| Avg. ppl = 16.63| Speed 2532.65 words/sec| Time 3.40 min|\n","| Epoch 2, Iter 500| Avg Loss = 68.69| Avg. ppl = 19.11| Speed 3578.52 words/sec| Time 3.74 min|\n","| Epoch 2, Iter 550| Avg Loss = 64.78| Avg. ppl = 17.00| Speed 3753.37 words/sec| Time 4.07 min|\n","| Epoch 2, Iter 600| Avg Loss = 64.86| Avg. ppl = 17.20| Speed 3588.61 words/sec| Time 4.41 min|\n","| <Train Summary> | Epoch 2, Iter 600| Cum. loss = 66.50| Cum. ppl = 17.46|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 17.454501\n","Save currently the best model to [Chatbot]\n","save model parameters to [Chatbot]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 2, Iter 650| Avg Loss = 63.65| Avg. ppl = 16.49| Speed 2360.03 words/sec| Time 4.92 min|\n","| Epoch 2, Iter 700| Avg Loss = 64.50| Avg. ppl = 16.94| Speed 3464.54 words/sec| Time 5.27 min|\n","| Epoch 2, Iter 750| Avg Loss = 63.30| Avg. ppl = 16.55| Speed 3598.84 words/sec| Time 5.61 min|\n","| Epoch 2, Iter 800| Avg Loss = 66.75| Avg. ppl = 14.60| Speed 3715.08 words/sec| Time 5.96 min|\n","| <Train Summary> | Epoch 2, Iter 800| Cum. loss = 64.55| Cum. ppl = 16.08|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 16.741361\n","Save currently the best model to [Chatbot]\n","save model parameters to [Chatbot]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 2, Iter 850| Avg Loss = 64.42| Avg. ppl = 12.88| Speed 2658.11 words/sec| Time 6.47 min|\n","| Epoch 2, Iter 900| Avg Loss = 64.19| Avg. ppl = 16.41| Speed 3635.67 words/sec| Time 6.81 min|\n","| Epoch 2, Iter 950| Avg Loss = 64.73| Avg. ppl = 15.17| Speed 3789.12 words/sec| Time 7.14 min|\n","| Epoch 3, Iter 1000| Avg Loss = 64.85| Avg. ppl = 14.61| Speed 3776.87 words/sec| Time 7.48 min|\n","| <Train Summary> | Epoch 3, Iter 1000| Cum. loss = 64.55| Cum. ppl = 14.67|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 16.149976\n","Save currently the best model to [Chatbot]\n","save model parameters to [Chatbot]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 3, Iter 1050| Avg Loss = 60.11| Avg. ppl = 14.09| Speed 2373.04 words/sec| Time 7.99 min|\n","| Epoch 3, Iter 1100| Avg Loss = 61.48| Avg. ppl = 14.23| Speed 3587.88 words/sec| Time 8.34 min|\n","| Epoch 3, Iter 1150| Avg Loss = 61.64| Avg. ppl = 13.58| Speed 3763.18 words/sec| Time 8.67 min|\n","| Epoch 3, Iter 1200| Avg Loss = 61.20| Avg. ppl = 14.35| Speed 3647.94 words/sec| Time 9.01 min|\n","| <Train Summary> | Epoch 3, Iter 1200| Cum. loss = 61.11| Cum. ppl = 14.06|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 16.007677\n","Save currently the best model to [Chatbot]\n","save model parameters to [Chatbot]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 3, Iter 1250| Avg Loss = 61.04| Avg. ppl = 14.24| Speed 2395.46 words/sec| Time 9.52 min|\n","| Epoch 3, Iter 1300| Avg Loss = 60.24| Avg. ppl = 11.09| Speed 3995.07 words/sec| Time 9.85 min|\n","| Epoch 3, Iter 1350| Avg Loss = 61.85| Avg. ppl = 11.75| Speed 3847.78 words/sec| Time 10.20 min|\n","| Epoch 3, Iter 1400| Avg Loss = 61.87| Avg. ppl = 14.89| Speed 3594.53 words/sec| Time 10.54 min|\n","| <Train Summary> | Epoch 3, Iter 1400| Cum. loss = 61.25| Cum. ppl = 12.82|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 15.765635\n","Save currently the best model to [Chatbot]\n","save model parameters to [Chatbot]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 3, Iter 1450| Avg Loss = 60.97| Avg. ppl = 14.20| Speed 2403.45 words/sec| Time 11.05 min|\n","| Epoch 4, Iter 1500| Avg Loss = 58.42| Avg. ppl = 12.58| Speed 3554.15 words/sec| Time 11.40 min|\n","| Epoch 4, Iter 1550| Avg Loss = 57.72| Avg. ppl = 11.00| Speed 3938.36 words/sec| Time 11.72 min|\n","| Epoch 4, Iter 1600| Avg Loss = 58.72| Avg. ppl = 11.64| Speed 3715.65 words/sec| Time 12.07 min|\n","| <Train Summary> | Epoch 4, Iter 1600| Cum. loss = 58.96| Cum. ppl = 12.28|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 15.749348\n","Save currently the best model to [Chatbot]\n","save model parameters to [Chatbot]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 4, Iter 1650| Avg Loss = 57.23| Avg. ppl = 12.49| Speed 2321.55 words/sec| Time 12.59 min|\n","| Epoch 4, Iter 1700| Avg Loss = 58.16| Avg. ppl = 12.98| Speed 3743.21 words/sec| Time 12.91 min|\n","| Epoch 4, Iter 1750| Avg Loss = 58.27| Avg. ppl = 11.45| Speed 3652.58 words/sec| Time 13.26 min|\n","| Epoch 4, Iter 1800| Avg Loss = 59.13| Avg. ppl = 13.12| Speed 3580.76 words/sec| Time 13.60 min|\n","| <Train Summary> | Epoch 4, Iter 1800| Cum. loss = 58.20| Cum. ppl = 12.48|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 15.621592\n","Save currently the best model to [Chatbot]\n","save model parameters to [Chatbot]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 4, Iter 1850| Avg Loss = 59.42| Avg. ppl = 12.21| Speed 2442.71 words/sec| Time 14.12 min|\n","| Epoch 4, Iter 1900| Avg Loss = 59.26| Avg. ppl = 11.34| Speed 3946.99 words/sec| Time 14.45 min|\n","| Epoch 4, Iter 1950| Avg Loss = 59.23| Avg. ppl = 13.27| Speed 3590.88 words/sec| Time 14.79 min|\n","| Epoch 5, Iter 2000| Avg Loss = 56.19| Avg. ppl = 11.19| Speed 3585.72 words/sec| Time 15.14 min|\n","| <Train Summary> | Epoch 5, Iter 2000| Cum. loss = 58.53| Cum. ppl = 11.96|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 15.620355\n","Save currently the best model to [Chatbot]\n","save model parameters to [Chatbot]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 5, Iter 2050| Avg Loss = 56.61| Avg. ppl = 11.26| Speed 2424.96 words/sec| Time 15.65 min|\n","| Epoch 5, Iter 2100| Avg Loss = 56.70| Avg. ppl = 11.41| Speed 3599.87 words/sec| Time 15.99 min|\n","| Epoch 5, Iter 2150| Avg Loss = 55.11| Avg. ppl = 11.24| Speed 3530.03 words/sec| Time 16.34 min|\n","| Epoch 5, Iter 2200| Avg Loss = 55.68| Avg. ppl = 11.47| Speed 3532.32 words/sec| Time 16.68 min|\n","| <Train Summary> | Epoch 5, Iter 2200| Cum. loss = 56.03| Cum. ppl = 11.34|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 15.627443\n","Hit patience 1\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 5, Iter 2250| Avg Loss = 56.91| Avg. ppl = 9.94| Speed 2561.98 words/sec| Time 17.20 min|\n","| Epoch 5, Iter 2300| Avg Loss = 56.14| Avg. ppl = 11.80| Speed 3680.10 words/sec| Time 17.53 min|\n","| Epoch 5, Iter 2350| Avg Loss = 56.71| Avg. ppl = 9.83| Speed 3867.91 words/sec| Time 17.87 min|\n","| Epoch 5, Iter 2400| Avg Loss = 56.02| Avg. ppl = 11.99| Speed 3695.13 words/sec| Time 18.20 min|\n","| <Train Summary> | Epoch 5, Iter 2400| Cum. loss = 56.45| Cum. ppl = 10.80|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 15.540236\n","Save currently the best model to [Chatbot]\n","save model parameters to [Chatbot]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 6, Iter 2450| Avg Loss = 54.63| Avg. ppl = 10.01| Speed 2514.24 words/sec| Time 18.70 min|\n","| Epoch 6, Iter 2500| Avg Loss = 52.90| Avg. ppl = 8.85| Speed 3862.99 words/sec| Time 19.03 min|\n","| Epoch 6, Iter 2550| Avg Loss = 52.71| Avg. ppl = 9.22| Speed 3697.61 words/sec| Time 19.38 min|\n","| Epoch 6, Iter 2600| Avg Loss = 54.45| Avg. ppl = 10.49| Speed 3627.40 words/sec| Time 19.72 min|\n","| <Train Summary> | Epoch 6, Iter 2600| Cum. loss = 53.67| Cum. ppl = 9.61|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 15.843303\n","Hit patience 1\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 6, Iter 2650| Avg Loss = 54.08| Avg. ppl = 10.55| Speed 2437.36 words/sec| Time 20.22 min|\n","| Epoch 6, Iter 2700| Avg Loss = 55.25| Avg. ppl = 10.53| Speed 3681.11 words/sec| Time 20.56 min|\n","| Epoch 6, Iter 2750| Avg Loss = 54.23| Avg. ppl = 10.69| Speed 3591.27 words/sec| Time 20.90 min|\n","| Epoch 6, Iter 2800| Avg Loss = 55.49| Avg. ppl = 10.88| Speed 3554.68 words/sec| Time 21.25 min|\n","| <Train Summary> | Epoch 6, Iter 2800| Cum. loss = 54.76| Cum. ppl = 10.66|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 15.808691\n","Hit patience 2\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 6, Iter 2850| Avg Loss = 53.67| Avg. ppl = 9.40| Speed 2542.07 words/sec| Time 21.75 min|\n","| Epoch 6, Iter 2900| Avg Loss = 54.46| Avg. ppl = 9.51| Speed 3860.67 words/sec| Time 22.08 min|\n","| Epoch 7, Iter 2950| Avg Loss = 53.07| Avg. ppl = 9.94| Speed 3561.85 words/sec| Time 22.43 min|\n","| Epoch 7, Iter 3000| Avg Loss = 50.39| Avg. ppl = 9.19| Speed 3624.63 words/sec| Time 22.76 min|\n","| <Train Summary> | Epoch 7, Iter 3000| Cum. loss = 52.90| Cum. ppl = 9.51|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 16.186788\n","Hit patience 3\n","Hit #1 trial\n","load previously best model and decay learning rate to 0.000500\n","restore parameters of the optimizers\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 7, Iter 3050| Avg Loss = 51.71| Avg. ppl = 7.50| Speed 2707.55 words/sec| Time 23.27 min|\n","| Epoch 7, Iter 3100| Avg Loss = 51.99| Avg. ppl = 10.00| Speed 3669.62 words/sec| Time 23.60 min|\n","| Epoch 7, Iter 3150| Avg Loss = 53.83| Avg. ppl = 9.37| Speed 3576.78 words/sec| Time 23.96 min|\n","| Epoch 7, Iter 3200| Avg Loss = 52.98| Avg. ppl = 10.11| Speed 3587.59 words/sec| Time 24.30 min|\n","| <Train Summary> | Epoch 7, Iter 3200| Cum. loss = 52.63| Cum. ppl = 9.13|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 15.557750\n","Hit patience 1\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 7, Iter 3250| Avg Loss = 51.90| Avg. ppl = 9.94| Speed 2386.95 words/sec| Time 24.80 min|\n","| Epoch 7, Iter 3300| Avg Loss = 53.00| Avg. ppl = 8.83| Speed 3829.29 words/sec| Time 25.14 min|\n","| Epoch 7, Iter 3350| Avg Loss = 53.79| Avg. ppl = 10.19| Speed 3742.18 words/sec| Time 25.47 min|\n","| Epoch 7, Iter 3400| Avg Loss = 53.82| Avg. ppl = 10.23| Speed 3684.72 words/sec| Time 25.81 min|\n","| <Train Summary> | Epoch 7, Iter 3400| Cum. loss = 53.13| Cum. ppl = 9.76|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 15.548370\n","Hit patience 2\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 8, Iter 3450| Avg Loss = 51.85| Avg. ppl = 9.40| Speed 2425.42 words/sec| Time 26.31 min|\n","| Epoch 8, Iter 3500| Avg Loss = 51.47| Avg. ppl = 9.27| Speed 3534.28 words/sec| Time 26.66 min|\n","| Epoch 8, Iter 3550| Avg Loss = 50.75| Avg. ppl = 8.10| Speed 3778.06 words/sec| Time 27.00 min|\n","| Epoch 8, Iter 3600| Avg Loss = 50.76| Avg. ppl = 9.25| Speed 3594.48 words/sec| Time 27.34 min|\n","| <Train Summary> | Epoch 8, Iter 3600| Cum. loss = 51.21| Cum. ppl = 8.97|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 15.853365\n","Hit patience 3\n","Hit #2 trial\n","load previously best model and decay learning rate to 0.000250\n","restore parameters of the optimizers\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 8, Iter 3650| Avg Loss = 53.17| Avg. ppl = 9.00| Speed 2539.04 words/sec| Time 27.85 min|\n","| Epoch 8, Iter 3700| Avg Loss = 53.04| Avg. ppl = 9.35| Speed 3722.53 words/sec| Time 28.19 min|\n","| Epoch 8, Iter 3750| Avg Loss = 52.49| Avg. ppl = 9.64| Speed 3603.05 words/sec| Time 28.53 min|\n","| Epoch 8, Iter 3800| Avg Loss = 51.38| Avg. ppl = 8.56| Speed 3725.49 words/sec| Time 28.88 min|\n","| <Train Summary> | Epoch 8, Iter 3800| Cum. loss = 52.52| Cum. ppl = 9.13|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 15.398778\n","Save currently the best model to [Chatbot]\n","save model parameters to [Chatbot]\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 8, Iter 3850| Avg Loss = 52.10| Avg. ppl = 9.25| Speed 2506.65 words/sec| Time 29.37 min|\n","| Epoch 8, Iter 3900| Avg Loss = 52.70| Avg. ppl = 9.90| Speed 3544.02 words/sec| Time 29.72 min|\n","| Epoch 9, Iter 3950| Avg Loss = 49.89| Avg. ppl = 9.29| Speed 3421.48 words/sec| Time 30.07 min|\n","| Epoch 9, Iter 4000| Avg Loss = 52.14| Avg. ppl = 9.41| Speed 3554.39 words/sec| Time 30.42 min|\n","| <Train Summary> | Epoch 9, Iter 4000| Cum. loss = 51.71| Cum. ppl = 9.46|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 15.505146\n","Hit patience 1\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 9, Iter 4050| Avg Loss = 51.29| Avg. ppl = 9.32| Speed 2401.64 words/sec| Time 30.93 min|\n","| Epoch 9, Iter 4100| Avg Loss = 51.84| Avg. ppl = 7.25| Speed 4163.84 words/sec| Time 31.26 min|\n","| Epoch 9, Iter 4150| Avg Loss = 51.96| Avg. ppl = 9.59| Speed 3650.21 words/sec| Time 31.60 min|\n","| Epoch 9, Iter 4200| Avg Loss = 50.09| Avg. ppl = 9.27| Speed 3648.19 words/sec| Time 31.93 min|\n","| <Train Summary> | Epoch 9, Iter 4200| Cum. loss = 51.29| Cum. ppl = 8.75|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 15.556466\n","Hit patience 2\n"],"name":"stderr"},{"output_type":"stream","text":["| Epoch 9, Iter 4250| Avg Loss = 51.83| Avg. ppl = 8.05| Speed 2594.20 words/sec| Time 32.44 min|\n","| Epoch 9, Iter 4300| Avg Loss = 51.55| Avg. ppl = 9.59| Speed 3745.79 words/sec| Time 32.76 min|\n","| Epoch 9, Iter 4350| Avg Loss = 53.33| Avg. ppl = 9.60| Speed 3562.13 words/sec| Time 33.12 min|\n","| Epoch 10, Iter 4400| Avg Loss = 51.87| Avg. ppl = 9.36| Speed 3530.25 words/sec| Time 33.47 min|\n","| <Train Summary> | Epoch 10, Iter 4400| Cum. loss = 52.15| Cum. ppl = 9.11|\n"],"name":"stdout"},{"output_type":"stream","text":["Report on validation set:\n","Validation:  Dev. ppl = 15.574506\n","Hit patience 3\n","Hit #3 trial\n","early stop!\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"EtcXIJ4wTV4l","colab_type":"text"},"source":["### Model reload"]},{"cell_type":"code","metadata":{"id":"WywotWQNBH3d","colab_type":"code","outputId":"14829c57-dd40-413a-c351-11e5b8b571cb","executionInfo":{"status":"ok","timestamp":1573706153223,"user_tz":-480,"elapsed":4671,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":585}},"source":["# load model\n","params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n","model.load_state_dict(params['state_dict'])\n","model = model.to(device)\n","\n","print('restore parameters of the optimizers', file=sys.stderr)\n","optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n","model.eval()"],"execution_count":54,"outputs":[{"output_type":"stream","text":["restore parameters of the optimizers\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["Chatbot(\n","  (master_embeddings): ModelEmbeddings(\n","    (LUT): Embedding(7504, 256, padding_idx=0)\n","  )\n","  (encoder): ChatbotEncoder(\n","    (embed): ModelEmbeddings(\n","      (LUT): Embedding(7504, 256, padding_idx=0)\n","    )\n","    (lstm): LSTM(256, 256, bidirectional=True)\n","    (dropout): Dropout(p=0.3, inplace=False)\n","    (h_projection): Linear(in_features=512, out_features=256, bias=False)\n","    (c_projection): Linear(in_features=512, out_features=256, bias=False)\n","  )\n","  (attn_mech): GlobalAttention(\n","    (mult_atten): Linear(in_features=256, out_features=256, bias=True)\n","  )\n","  (decoder): ChatbotDecoder(\n","    (attention): GlobalAttention(\n","      (mult_atten): Linear(in_features=256, out_features=256, bias=True)\n","    )\n","    (embed): ModelEmbeddings(\n","      (LUT): Embedding(7504, 256, padding_idx=0)\n","    )\n","    (lstm_decode): LSTMCell(512, 256)\n","    (att_projection): Linear(in_features=512, out_features=256, bias=False)\n","    (combined_output_projection): Linear(in_features=768, out_features=256, bias=False)\n","    (dropout): Dropout(p=0.3, inplace=False)\n","  )\n","  (vocab_projection): Linear(in_features=256, out_features=7504, bias=False)\n","  (dropout): Dropout(p=0.3, inplace=False)\n",")"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"v6a0DfbuBH1m","colab_type":"code","outputId":"88cd7c65-3c63-413b-a9d4-39d3ab1dad95","executionInfo":{"status":"ok","timestamp":1573706161404,"user_tz":-480,"elapsed":4134,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# check validation ppl\n","evaluate_ppl(model, valid_data, batch_size=128)"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15.398778353470794"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"tZfxsJIYUP95","colab_type":"text"},"source":["### Beam search"]},{"cell_type":"code","metadata":{"id":"j6Pafsi0lJSc","colab_type":"code","colab":{}},"source":["Hypothesis = namedtuple('Hypothesis', ['value', 'score'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OPWWE2ICxe6","colab_type":"code","colab":{}},"source":["class BeamSearch(nn.Module):\n","  def __init__(self, chatbot, beam_size = 5, max_steps = 70):\n","    \n","    super(BeamSearch, self).__init__()\n","    self.model = chatbot\n","    self.beam_size = beam_size\n","    self.max_steps = max_steps\n","  \n","  \n","  def forward(self, src_sent):\n","    \n","    src_sent = [word for word in src_sent.split(\" \")]\n","    src_sents_var = self.model.vocab.to_input_tensor([src_sent], self.device)\n","    src_encodings, enc_masks, dec_init_vec = self.model.encoder(src_sents_var, [len(src_sent)])\n","    src_encodings_att_linear = self.model.decoder.att_projection(src_encodings)\n","    h_tm1 = dec_init_vec\n","    att_tm1 = torch.zeros(1, self.model.hidden_size, device=self.device)\n","    eos_id = self.model.vocab['<eos>']\n","\n","    hypotheses = [['<sos>']]\n","    hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n","    completed_hypotheses = []\n","    t = 0\n","    while len(completed_hypotheses) < self.beam_size and t < self.max_steps:\n","      t += 1\n","      hyp_num = len(hypotheses)\n","      exp_src_encodings = src_encodings.expand(hyp_num,\n","                                                src_encodings.size(1),\n","                                                src_encodings.size(2))\n","\n","      exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n","                                                                      src_encodings_att_linear.size(1),\n","                                                                      src_encodings_att_linear.size(2))\n","\n","      y_tm1 = torch.tensor([self.model.vocab[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n","      y_t_embed = self.model.decoder.embed(y_tm1)\n","\n","      x = torch.cat([y_t_embed, att_tm1], dim=-1)\n","      \n","      #(h_t, cell_t), att_t, _  = self.step(x, h_tm1,\n","      #                                          exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n","      \n","      \n","      (h_t, cell_t) = self.model.decoder.lstm_decode(x, h_tm1) # tuple of (dec_hidden, dec_cell) Tensors of shape (batch, hidden_size)\n","      #dec_hidden, dec_cell = dec_state # (batch, hidden_size)\n","\n","      # Global Attention Mechanisms\n","      alpha_t = self.model.decoder.attention(exp_src_encodings_att_linear, h_tm1[0]) \n","      \n","      \n","      # Compute attention vector\n","      aug_att = torch.unsqueeze(alpha_t,2) #(batch, src_len, 1)\n","      tr_hiddens = exp_src_encodings.transpose(1,2) #(batch, hidden_size*2, src_len)\n","      a_t = torch.bmm(tr_hiddens, aug_att) #(batch, 2*hidden_size, 1)\n","      a_t = torch.squeeze(a_t, dim=2) #(batch, 2*hidden_size) # Attention vector\n","      \n","      # Compute combined output\n","      U_t = torch.cat((a_t, h_t), dim=1) #(batch, 3*hidden_size)\n","      V_t = self.model.decoder.combined_output_projection(U_t) #(batch, hidden_size)\n","      O_t = self.model.decoder.dropout(torch.tanh(V_t)) # (batch, hidden_size)\n","\n","\n","\n","\n","      \n","      # log probabilities over target words\n","      log_p_t = F.log_softmax(self.model.vocab_projection(O_t), dim=-1)\n","      \n","      live_hyp_num = self.beam_size - len(completed_hypotheses)\n","      contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n","      top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n","      prev_hyp_ids = top_cand_hyp_pos / len(self.model.vocab)\n","      hyp_word_ids = top_cand_hyp_pos % len(self.model.vocab)\n","      \n","      new_hypotheses = []\n","      live_hyp_ids = []\n","      new_hyp_scores = []\n","\n","      #set_trace()\n","\n","      for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n","        prev_hyp_id = prev_hyp_id.item()\n","        hyp_word_id = hyp_word_id.item()\n","        cand_new_hyp_score = cand_new_hyp_score.item()\n","\n","        hyp_word = self.model.vocab.id2word[hyp_word_id]\n","        new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n","        if hyp_word == '<eos>':\n","          completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1], score=cand_new_hyp_score))\n","        else:\n","          new_hypotheses.append(new_hyp_sent)\n","          live_hyp_ids.append(prev_hyp_id)\n","          new_hyp_scores.append(cand_new_hyp_score)\n","      if len(completed_hypotheses) == self.beam_size:\n","        break\n","      live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n","      h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n","      att_tm1 = O_t[live_hyp_ids]\n","      \n","      hypotheses = new_hypotheses\n","      hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n","    \n","    \n","    if len(completed_hypotheses) == 0:\n","      completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:], score=hyp_scores[0].item()))\n","    completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n","    return completed_hypotheses\n","\n","  \n","  @property\n","  def device(self) -> torch.device:\n","      \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n","      \"\"\"\n","      return self.model.master_embeddings.LUT.weight.device\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewg1KypAXB_g","colab_type":"code","outputId":"d1094b4c-5651-44c7-c15a-45e8a5fdf4fd","executionInfo":{"status":"ok","timestamp":1573706170138,"user_tz":-480,"elapsed":1159,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["beamBot = BeamSearch(model)\n","src_sent = 'monica: how are you?'\n","x = beamBot(src_sent)\n","x"],"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Hypothesis(value=['joey', ':', '', 'hey', '!', ''], score=-6.241628646850586)]"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"markdown","metadata":{"id":"Opuprt5CscDl","colab_type":"text"},"source":["### Inference"]},{"cell_type":"code","metadata":{"id":"zs0nys10seQy","colab_type":"code","outputId":"de668624-9c04-4d61-87dc-3092a8f258f6","executionInfo":{"status":"ok","timestamp":1573708333767,"user_tz":-480,"elapsed":2094,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["####\n","Q = 'ross: ( entering room )'\n","A = beamBot(Q)\n","\n","print(\"==\"*40)\n","print(Q)\n","print(\"Bot:\\n\")\n","print('{}'.format(' '.join(A[0].value)))\n","print('{}'.format(' '.join(A[1].value)))\n","print(\"==\"*40)"],"execution_count":79,"outputs":[{"output_type":"stream","text":["================================================================================\n","ross: ( entering room )\n","Bot:\n","\n","joey :  hey ! \n","joey :  hey ,  hey ! \n","================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QtIc4AqXVSHJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":230},"outputId":"44834690-ac2f-4975-8488-dc2bc46aa2eb","executionInfo":{"status":"ok","timestamp":1573708617801,"user_tz":-480,"elapsed":1174,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}}},"source":["valid_data[idx][1]"],"execution_count":95,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<sos>',\n"," 'monica',\n"," ':',\n"," '',\n"," 'you',\n"," 'do',\n"," 'not',\n"," 'know',\n"," 'that',\n"," '.',\n"," '',\n"," '<eos>']"]},"metadata":{"tags":[]},"execution_count":95}]},{"cell_type":"code","metadata":{"id":"TCGw_x-pseYq","colab_type":"code","outputId":"ff3a1133-cc71-473f-9a12-dd7fa1141b52","executionInfo":{"status":"ok","timestamp":1573708641122,"user_tz":-480,"elapsed":1186,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["####\n","idx = 4\n","Q = \" \".join(valid_data[idx][0])\n","A_tr = \" \".join(valid_data[idx][1])\n","\n","# Q = 'rachel :  really ? '\n","A = beamBot(Q)\n","\n","print(\"==\"*40)\n","print(Q)\n","print(\"Bot:\\n\")\n","print('{}'.format(' '.join(A[0].value)))\n","print('{}'.format(A_tr))\n","print(\"==\"*40)"],"execution_count":96,"outputs":[{"output_type":"stream","text":["================================================================================\n","chandler :  you know what just occurred to me ?  this could be our last thanksgiving just the two of us .  i mean ,  we could be getting a baby soon ! \n","Bot:\n","\n","monica :   ( entering )  hey ! \n","<sos> monica :  you do not know that .  <eos>\n","================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hqMbqNlhseW-","colab_type":"code","outputId":"b293fb9f-2526-4d01-c9ee-a9b0bf9e5f8a","executionInfo":{"status":"ok","timestamp":1573706196231,"user_tz":-480,"elapsed":1165,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["####\n","Q = 'ross : why are you not ready yet'\n","A = beamBot(Q)\n","\n","print(\"==\"*40)\n","print(Q)\n","print(\"Bot:\\n\")\n","print('{}'.format(' '.join(A[0].value)))\n","print('{}'.format(' '.join(A[1].value)))\n","print(\"==\"*40)"],"execution_count":61,"outputs":[{"output_type":"stream","text":["================================================================================\n","ross : why are you not ready yet\n","Bot:\n","\n","phoebe :  i know . \n","phoebe :  i don’t know . \n","================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mgzvp4C_seVV","colab_type":"code","outputId":"016680a2-5ba8-4c3e-ce15-a3cc3c76f9dd","executionInfo":{"status":"ok","timestamp":1573706218294,"user_tz":-480,"elapsed":1198,"user":{"displayName":"Saun Walker","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2JBKbx-BGs4LvdNR1zoWCRTfYpr9z2GYtDRpEEQ=s64","userId":"15930704027125169268"}},"colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["####\n","Q = \"why\"\n","A = beamBot(Q)\n","\n","print(\"==\"*40)\n","print(Q)\n","print(\"Bot:\\n\")\n","print('{}'.format(' '.join(A[0].value)))\n","print('{}'.format(' '.join(A[1].value)))\n","print('{}'.format(' '.join(A[2].value)))\n","print('{}'.format(' '.join(A[3].value)))\n","print(\"==\"*40)"],"execution_count":63,"outputs":[{"output_type":"stream","text":["================================================================================\n","why\n","Bot:\n","\n","rachel\n","chandler\n","monica\n","rachel : \n","================================================================================\n"],"name":"stdout"}]}]}